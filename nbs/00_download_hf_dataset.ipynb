{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "from loguru import logger\n",
    "from pathlib import Path\n",
    "\n",
    "dataset = load_dataset(\"frgfm/imagenette\", \"full_size\")\n",
    "\n",
    "active_learning_dataset = DatasetDict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tench',\n",
       " 'English springer',\n",
       " 'cassette player',\n",
       " 'chain saw',\n",
       " 'church',\n",
       " 'French horn',\n",
       " 'garbage truck',\n",
       " 'gas pump',\n",
       " 'golf ball',\n",
       " 'parachute']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['validation'].features['label'].names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save image to disk and add filepath to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function save_image at 0x74eec5768d60> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11d3647213b04ab68c221b26b11db73e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving evaluation images (num_proc=8):   0%|          | 0/3925 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55f609e769044cb3a6fbe9b91816397d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving unlabeled images (num_proc=8):   0%|          | 0/9469 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Constants\n",
    "SAVE_DIR = Path(\"data/imagenette\")\n",
    "NUM_PROC = 8  # Adjust based on your CPU\n",
    "CLASS_NAMES = dataset['validation'].features['label'].names\n",
    "\n",
    "def save_image(example, idx):\n",
    "    \"\"\"Save a dataset image to disk with error handling and add label name.\n",
    "    \n",
    "    Args:\n",
    "        example (dict): Dataset example containing 'image' and 'label'\n",
    "        idx (int): Index of the example\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary containing the saved filepath and label_name\n",
    "    \"\"\"\n",
    "    try:\n",
    "        image = example['image']\n",
    "        label_id = example.get('label')  # Get label if it exists (won't exist for unlabeled)\n",
    "        \n",
    "        # Get label name if label exists\n",
    "        label_name = CLASS_NAMES[label_id] if label_id is not None else None\n",
    "        \n",
    "        # Create directory structure\n",
    "        label_dir = SAVE_DIR / str(label_id if label_id is not None else 'unlabeled')\n",
    "        label_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Create filename with label subdirectory\n",
    "        filepath = label_dir / f\"{idx:05d}.jpg\"\n",
    "        \n",
    "        # Save with quality optimization\n",
    "        image.save(filepath, \"JPEG\", quality=95, optimize=True)\n",
    "        \n",
    "        return {\n",
    "            \"filepath\": str(filepath),\n",
    "            \"label_name\": label_name\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error saving image {idx}: {str(e)}\")\n",
    "        return {\n",
    "            \"filepath\": None,\n",
    "            \"label_name\": None\n",
    "        }\n",
    "\n",
    "\n",
    "active_learning_dataset['evaluation'] = dataset['validation'].map(\n",
    "    save_image,\n",
    "    with_indices=True,\n",
    "    num_proc=NUM_PROC,\n",
    "    desc=\"Saving evaluation images\",\n",
    "    remove_columns=\"label\"\n",
    ")\n",
    "\n",
    "active_learning_dataset['unlabeled'] = dataset['train'].map(\n",
    "    save_image,\n",
    "    with_indices=True,\n",
    "    num_proc=NUM_PROC,\n",
    "    desc=\"Saving unlabeled images\",\n",
    "    remove_columns=\"label\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=500x487>,\n",
       " 'filepath': 'data/imagenette/2/00000.jpg',\n",
       " 'label_name': 'cassette player'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "active_learning_dataset['evaluation'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=500x281>,\n",
       " 'filepath': 'data/imagenette/2/00000.jpg',\n",
       " 'label_name': 'cassette player'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "active_learning_dataset['unlabeled'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_learning_dataset['unlabeled'] = active_learning_dataset['unlabeled'].remove_columns('label_name')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    evaluation: Dataset({\n",
       "        features: ['image', 'filepath', 'label_name'],\n",
       "        num_rows: 3925\n",
       "    })\n",
       "    unlabeled: Dataset({\n",
       "        features: ['image', 'filepath'],\n",
       "        num_rows: 9469\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "active_learning_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e3e3f1ffdda4ffb90648e041cbe6bd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c8eeaee8c134c39b04b0a5dd8ba2e30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4735 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe10843fcad54e6bb8889b99c48365fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/48 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "298b6d94a9d04bdda3d889735552ff71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4734 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58646fd0c2854adfb7bc76e2fb99129d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/48 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "322070a1e11046e7bc9886ec21ae9ed5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b14ab9aed7645ebb380b4f4a371e426",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3925 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13a6312e7217406b8150b8d3a82ba95c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/40 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8612594f04345a38dd822906d313eeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/357 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/dnth/active-learning-imagenette/commit/9befdc61347df8562af6ebce44e7db455dd605f6', commit_message='Upload dataset', commit_description='', oid='9befdc61347df8562af6ebce44e7db455dd605f6', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/dnth/active-learning-imagenette', endpoint='https://huggingface.co', repo_type='dataset', repo_id='dnth/active-learning-imagenette'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "active_learning_dataset[\"unlabeled\"].push_to_hub(\"dnth/active-learning-imagenette\", \"unlabeled\")\n",
    "active_learning_dataset[\"evaluation\"].push_to_hub(\"dnth/active-learning-imagenette\", \"evaluation\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
