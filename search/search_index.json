{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>The goal of this project is to create a framework for the active learning loop for computer vision. The diagram below shows a general workflow of how the active learning loop works.</p> <p> </p>"},{"location":"#supported-tasks","title":"Supported tasks:","text":"<ul> <li>[X] Image classification</li> <li>[ ] Object detection</li> <li>[ ] Segmentation</li> </ul>"},{"location":"#supported-models","title":"Supported models:","text":"<ul> <li>[X] Fastai models</li> <li>[X] Torchvision models</li> <li>[X] Timm models</li> <li>[ ] Hugging Face models</li> </ul>"},{"location":"#supported-active-learning-strategies","title":"Supported Active Learning Strategies:","text":"<p>Uncertainty Sampling: - [X] Least confidence - [X] Margin of confidence - [X] Ratio of confidence - [X] Entropy</p> <p>Diverse Sampling: - [X] Random sampling - [X] Model-based outlier - [ ] Embeddings-based outlier - [ ] Cluster-based - [ ] Representative</p>"},{"location":"#installation","title":"\ud83d\udce6 Installation","text":"<p>Get a release from PyPI <pre><code>pip install active-vision\n</code></pre></p> <p>Install from source <pre><code>git clone https://github.com/dnth/active-vision.git\ncd active-vision\npip install -e .\n</code></pre></p> <p>[!TIP] I recommend using uv to set up a virtual environment and install the package. You can also use other virtual env of your choice.</p> <p>If you're using uv:</p> <p><pre><code>uv venv\nuv sync\n</code></pre> Once the virtual environment is created, you can install the package using pip.</p> <p>If you're using uv add a <code>uv</code> before the pip install command to install into your virtual environment. Eg: <pre><code>uv pip install active-vision\n</code></pre></p>"},{"location":"#usage","title":"\ud83d\udee0\ufe0f Usage","text":"<p>See the notebook for a complete example.</p> <p>Be sure to prepared 3 subsets of the dataset: - Initial samples: A dataframe of a labeled images to train an initial model. If you don't have any labeled data, you can label some images yourself. - Unlabeled samples: A dataframe of unlabeled images. We will continuously sample from this set using active learning strategies. - Evaluation samples: A dataframe of labeled images. We will use this set to evaluate the performance of the model. This is the test set, DO NOT use it for active learning. Split this out in the beginning.</p> <p>As a toy example I created the above 3 datasets from the imagenette dataset.</p> <pre><code>from active_vision import ActiveLearner\nimport pandas as pd\n\n# Create an active learner instance with a model\nal = ActiveLearner(\"resnet18\")\n\n# Load dataset \ntrain_df = pd.read_parquet(\"training_samples.parquet\")\nal.load_dataset(df, filepath_col=\"filepath\", label_col=\"label\")\n\n# Train model\nal.train(epochs=3, lr=1e-3)\n\n# Evaluate the model on a *labeled* evaluation set\naccuracy = al.evaluate(eval_df, filepath_col=\"filepath\", label_col=\"label\")\n\n# Get predictions from an *unlabeled* set\npred_df = al.predict(filepaths)\n\n# Sample low confidence predictions from unlabeled set\nuncertain_df = al.sample_uncertain(pred_df, num_samples=10)\n\n# Launch a Gradio UI to label the low confidence samples, save the labeled samples to a file\nal.label(uncertain_df, output_filename=\"uncertain\")\n</code></pre> <p></p> <p>In the UI, you can optionally run zero-shot inference on the image. This will use a VLM to predict the label of the image. There are a dozen VLM models as supported in the x.infer project.</p> <p></p> <p>Once complete, the labeled samples will be save into a new df. We can now add the newly labeled data to the training set.</p> <pre><code># Add newly labeled data to training set and save as a new file active_labeled\nal.add_to_train_set(labeled_df, output_filename=\"active_labeled\")\n</code></pre> <p>Repeat the process until the model is good enough. Use the dataset to train a larger model and deploy.</p> <p>[!TIP] For the toy dataset, I got to about 93% accuracy on the evaluation set with 200+ labeled images. The best performing model on the leaderboard got 95.11% accuracy training on all 9469 labeled images.</p> <p>This took me about 6 iterations of relabeling. Each iteration took about 5 minutes to complete including labeling and model training (resnet18). See the notebook for more details.</p> <p>But using the dataset of 200+ images, I trained a more capable model (convnext_small_in22k) and got 99.3% accuracy on the evaluation set. See the notebook for more details.</p>"},{"location":"#benchmarks","title":"\ud83d\udcca Benchmarks","text":"<p>This section contains the benchmarks I ran using the active learning loop on various datasets.</p> <p>Column description: - <code>#Labeled Images</code>: The number of labeled images used to train the model. - <code>Evaluation Accuracy</code>: The accuracy of the model on the evaluation set. - <code>Train Epochs</code>: The number of epochs used to train the model. - <code>Model</code>: The model used to train. - <code>Active Learning</code>: Whether active learning was used to train the model. - <code>Source</code>: The source of the results.</p>"},{"location":"#imagenette","title":"Imagenette","text":"<ul> <li>num classes: 10</li> <li>num images: 9469</li> </ul> <p>To start the active learning loop, I labeled 100 images (10 images from each class) and iteratively relabeled the most informative images until I hit 275 labeled images. </p> <p>The active learning loop is a iterative process and can keep going until you hit a stopping point. You can decide your own stopping point based on your use case. It could be: - You ran out of data to label. - You hit a performance goal. - You hit a budget. - Other criteria.</p> <p>For this dataset, I decided to stop the active learning loop at 275 labeled images because the performance on the evaluation set exceeds the top performing model on the leaderboard. </p> #Labeled Images Evaluation Accuracy Train Epochs Model Active Learning Source 9469 94.90% 80 xse_resnext50 \u274c Link 9469 95.11% 200 xse_resnext50 \u274c Link 275 99.33% 6 convnext_small_in22k \u2713 Link 275 93.40% 4 resnet18 \u2713 Link"},{"location":"#dog-food","title":"Dog Food","text":"<ul> <li>num classes: 2</li> <li>num images: 2100</li> </ul> <p>To start the active learning loop, I labeled 20 images (10 images from each class) and iteratively relabeled the most informative images until I hit 160 labeled images. </p> <p>I decided to stop the active learning loop at 160 labeled images because the performance on the evaluation set is close to the top performing model on the leaderboard. You can decide your own stopping point based on your use case.</p> #Labeled Images Evaluation Accuracy Train Epochs Model Active Learning Source 2100 99.70% ? vit-base-patch16-224 \u274c Link 160 100.00% 6 convnext_small_in22k \u2713 Link 160 97.60% 4 resnet18 \u2713 Link"},{"location":"#oxford-iiit-pet","title":"Oxford-IIIT Pet","text":"<ul> <li>num classes: 37</li> <li>num images: 3680</li> </ul> <p>To start the active learning loop, I labeled 370 images (10 images from each class) and iteratively relabeled the most informative images until I hit 612 labeled images. </p> <p>I decided to stop the active learning loop at 612 labeled images because the performance on the evaluation set is close to the top performing model on the leaderboard. You can decide your own stopping point based on your use case.</p> #Labeled Images Evaluation Accuracy Train Epochs Model Active Learning Source 3680 95.40% 5 vit-base-patch16-224 \u274c Link 612 90.26% 11 convnext_small_in22k \u2713 Link 612 91.38% 11 vit-base-patch16-224 \u2713 Link"},{"location":"#eurosat-rgb","title":"Eurosat RGB","text":"<ul> <li>num classes: 10</li> <li>num images: 16100</li> </ul> <p>To start the active learning loop, I labeled 100 images (10 images from each class) and iteratively labeled the most informative images until I hit 1188 labeled images. </p> <p>I decided to stop the active learning loop at 1188 labeled images because the performance on the evaluation set is close to the top performing model on the leaderboard. You can decide your own stopping point based on your use case.</p> #Labeled Images Evaluation Accuracy Train Epochs Model Active Learning Source 16100 98.55% 6 vit-base-patch16-224 \u274c Link 1188 94.59% 6 vit-base-patch16-224 \u2713 Link 1188 96.57% 13 vit-base-patch16-224 \u2713 Link"},{"location":"#workflow","title":"\u27bf Workflow","text":"<p>This section describes a more detailed workflow for active learning. There are two workflows for active learning that we can use depending on the availability of labeled data.</p>"},{"location":"#with-unlabeled-data","title":"With unlabeled data","text":"<p>If we have no labeled data, the goal of the active learning loop is to build a resonably good labeled dataset to train a larger model.</p> <p>Steps:</p> <ol> <li>Load a small proxy model.</li> <li>Label an initial dataset. If there is none, you'll have to label some images.</li> <li>Train the proxy model on the labeled dataset.</li> <li>Run inference on the unlabeled dataset.</li> <li>Evaluate the performance of the proxy model.</li> <li>Is model good enough?    <ul> <li>Yes: Save the proxy model and the dataset.</li> <li>No: Select the most informative images to label using active learning.</li> </ul> </li> <li>Label the most informative images and add them to the dataset.</li> <li>Repeat steps 3-6.</li> <li>Save the proxy model and the dataset.</li> <li>Train a larger model on the saved dataset.</li> </ol> <pre><code>graph TD\n    A[Load a small proxy model] --&gt; B[Label an initial dataset]\n    B --&gt; C[Train proxy model on labeled dataset]\n    C --&gt; D[Run inference on unlabeled dataset]\n    D --&gt; E[Evaluate proxy model performance]\n    E --&gt; F{Model good enough?}\n    F --&gt;|Yes| G[Save proxy model and dataset]\n    G --&gt; H[Train and deploy a larger model]\n    F --&gt;|No| I[Select informative images using active learning]\n    I --&gt; J[Label selected images]\n    J --&gt; C\n</code></pre>"},{"location":"#with-labeled-data","title":"With labeled data","text":"<p>If we already have a labeled dataset, the goal of the active learning loop is to iteratively improve the dataset and the model by fixing the most important label errors.</p> <p>Steps:</p> <ol> <li>Load a small proxy model.</li> <li>Train the proxy model on the labeled dataset.</li> <li>Run inference on the entire labeled dataset.</li> <li>Get the most impactful label errors with active learning.</li> <li>Fix the label errors.</li> <li>Repeat steps 2-5 until the dataset is good enough.</li> <li>Save the labeled dataset.</li> <li>Train a larger model on the saved labeled dataset.</li> </ol> <pre><code>graph TD\n    A[Load a small proxy model] --&gt; B[Train proxy model on labeled dataset]\n    B --&gt; C[Run inference on labeled dataset]\n    C --&gt; D[Get label errors using active learning]\n    D --&gt; E[Fix label errors]\n    E --&gt; F{Dataset good enough?}\n    F --&gt;|No| B\n    F --&gt;|Yes| G[Save cleaned dataset]\n    G --&gt; H[Train and deploy larger model]\n</code></pre>"},{"location":"#sampling-approaches","title":"\ud83e\uddf1 Sampling Approaches","text":"<p>Recommendation 1: - 10% randomly selected from unlabeled items. - 80% selected from the lowest confidence items. - 10% selected as outliers.</p> <p>Recommendation 2:</p> <ul> <li>Sample 100 predicted images at 10\u201320% confidence.</li> <li>Sample 100 predicted images at 20\u201330% confidence.</li> <li>Sample 100 predicted images at 30\u201340% confidence, and so on.</li> </ul> <p>Uncertainty and diversity sampling are most effective when combined. For instance, you could first sample the most uncertain items using an uncertainty sampling method, then apply a diversity sampling method such as clustering to select a diverse set from the uncertain items.</p> <p>Ultimately, the right ratios can depend on the specific task and dataset.</p>"},{"location":"quickstart/","title":"Quickstart","text":""},{"location":"quickstart/#introduction","title":"Introduction","text":"<p>This notebook will guide you through the basic steps to get started with Active Vision.</p> <p>By the end of this notebook, you will be able to:</p> <ul> <li>Understand the basic workflow of active learning</li> <li>Understand the basic components of Active Vision</li> <li>Understand how to use Active Vision to train a model</li> <li>Understand how to use Active Vision to iteratively improve your dataset</li> </ul> <p>Before we start, we need to prepare 3 sets of data:</p> <ul> <li>Initial samples: A dataset of labeled images to train an initial model. If you don't have any labeled data, you can label some images yourself.</li> <li>Unlabeled samples: A dataset of unlabeled images. We will continuously sample from this set using active learning strategies.</li> <li>Evaluation samples: A dataset of labeled images. We will use this set to evaluate the performance of the model. This is the test set, DO NOT use it for active learning. Split this out in the beginning.</li> </ul> <p>We will use the Imagenette dataset as a working example in this notebook.</p>"},{"location":"quickstart/#load-the-dataset","title":"Load the dataset","text":"<p><code>active-vision</code> currently supports datasets in a pandas dataframe format. The dataframe should have at least 2 columns: <code>filepath</code> and <code>label</code>.</p> <pre><code>import pandas as pd\n\ninitial_samples = pd.read_parquet(\"imagenette/initial_samples.parquet\")\ninitial_samples.head()\n</code></pre> filepath label 0 data/imagenette/2/00710.jpg cassette player 1 data/imagenette/2/00063.jpg cassette player 2 data/imagenette/2/00506.jpg cassette player 3 data/imagenette/2/00575.jpg cassette player 4 data/imagenette/2/00136.jpg cassette player <p>Let's check the distribution of the labels.</p> <pre><code>initial_samples[\"label\"].value_counts()\n</code></pre> <pre><code>label\ncassette player     10\ntench               10\nchain saw           10\nchurch              10\nparachute           10\ngas pump            10\nEnglish springer    10\ngolf ball           10\ngarbage truck       10\nFrench horn         10\nName: count, dtype: int64\n</code></pre>"},{"location":"quickstart/#create-an-activelearner","title":"Create an ActiveLearner","text":"<p>Now that we have an initial dataset, we can load it into an <code>ActiveLearner</code> object with a model.</p> <p>Any fastai and timm models are supported. For simplicity, we will use a <code>resnet18</code> model.</p> <pre><code>from active_vision import ActiveLearner\nfrom fastai.vision.models.all import resnet18\n\nal = ActiveLearner(resnet18)\n</code></pre> <pre><code>2025-01-25 00:01:47.070 | INFO     | active_vision.core:load_model:41 - Loading fastai model resnet18\n</code></pre> <p>We can load the initial samples into the <code>ActiveLearner</code> object.</p> <pre><code>al.load_dataset(initial_samples, \n                filepath_col=\"filepath\", \n                label_col=\"label\", \n                batch_size=8)\n</code></pre> <pre><code>2025-01-25 00:01:47.075 | INFO     | active_vision.core:load_dataset:59 - Loading dataset from filepath and label\n2025-01-25 00:01:47.075 | INFO     | active_vision.core:load_dataset:61 - Creating dataloaders\n2025-01-25 00:01:47.321 | INFO     | active_vision.core:load_dataset:83 - Creating learner\n2025-01-25 00:01:47.473 | INFO     | active_vision.core:load_dataset:92 - Done. Ready to train.\n</code></pre> <pre><code>al.show_batch()\n</code></pre> <p></p> <p>You can inspect the train and validation sets too.</p> <pre><code>al.train_set.head()\n</code></pre> filepath label 51 data/imagenette/7/05378.jpg gas pump 83 data/imagenette/6/07797.jpg garbage truck 97 data/imagenette/5/08999.jpg French horn 75 data/imagenette/8/06618.jpg golf ball 9 data/imagenette/2/00420.jpg cassette player <pre><code>al.valid_set.head()\n</code></pre> filepath label 4 data/imagenette/2/00136.jpg cassette player 64 data/imagenette/1/06574.jpg English springer 40 data/imagenette/9/04652.jpg parachute 54 data/imagenette/7/05488.jpg gas pump 20 data/imagenette/3/02629.jpg chain saw"},{"location":"quickstart/#train","title":"Train","text":"<p>Now that we have the initial dataset, we can train the model.</p> <p>But first, let's check the optimal learning rate for the model.</p> <pre><code>al.lr_find()\n</code></pre> <pre><code>2025-01-25 00:01:48.042 | INFO     | active_vision.core:lr_find:115 - Finding optimal learning rate\n</code></pre> <pre><code>2025-01-25 00:01:54.456 | INFO     | active_vision.core:lr_find:117 - Optimal learning rate: 0.00363078061491251\n</code></pre> <p></p> <p>Not let's use the optimal learning rate to train the model end-to-end for 3 epochs and 1 epoch of head tuning.</p> <pre><code>al.train(epochs=3, lr=5e-3, head_tuning_epochs=1)\n</code></pre> <pre><code>2025-01-25 00:01:54.717 | INFO     | active_vision.core:train:128 - Training head for 1 epochs\n2025-01-25 00:01:54.718 | INFO     | active_vision.core:train:129 - Training model end-to-end for 3 epochs\n2025-01-25 00:01:54.718 | INFO     | active_vision.core:train:130 - Learning rate: 0.005 with one-cycle learning rate scheduler\n</code></pre> epoch train_loss valid_loss accuracy time 0 2.871501 0.771290 0.750000 00:01 <p></p> epoch train_loss valid_loss accuracy time 0 0.460057 0.409270 0.850000 00:01 1 0.370149 0.635701 0.800000 00:01 2 0.286742 0.720829 0.800000 00:01 <p></p>"},{"location":"quickstart/#evaluate","title":"Evaluate","text":"<p>Now that we have a trained model, we can evaluate it on the evaluation set.</p> <pre><code>evaluation_df = pd.read_parquet(\"imagenette/evaluation_samples.parquet\")\nevaluation_df.head()\n</code></pre> filepath label 0 data/imagenette/2/00000.jpg cassette player 1 data/imagenette/2/00001.jpg cassette player 2 data/imagenette/2/00002.jpg cassette player 3 data/imagenette/2/00003.jpg cassette player 4 data/imagenette/2/00004.jpg cassette player <pre><code>al.evaluate(evaluation_df, filepath_col=\"filepath\", label_col=\"label\")\n</code></pre> <pre><code>2025-01-25 00:02:04.166 | INFO     | active_vision.core:evaluate:183 - Accuracy: 89.22%\n\n\n\n\n\n0.8922292993630573\n</code></pre> <pre><code>\n</code></pre>"}]}